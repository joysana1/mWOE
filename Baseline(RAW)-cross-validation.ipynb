{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"D:\\Thesis\\BIG DATA\\Database Churn\\joy work\\Telecom_customer churn (100000).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = df.select_dtypes(include=['float64', 'int64']).copy()\n",
    "cat_df = df.select_dtypes(include=['object']).copy()\n",
    "\n",
    "# Categorical boolean mask\n",
    "categorical_feature_mask = cat_df.dtypes==object\n",
    "# filter categorical columns using mask and turn it into a list\n",
    "categorical_cols = cat_df.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "import numpy as np\n",
    "#conData=np.log(0.00001 + 1)\n",
    "conData=0\n",
    "cat_df=cat_df.fillna(conData)\n",
    "num_df=num_df.fillna(conData)\n",
    "cat_df=cat_df.astype(str)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "cat_df[categorical_cols] = cat_df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "#cat_df[categorical_cols].head(10)\n",
    "\n",
    "change_mou=num_df['change_mou']\n",
    "change_rev=num_df['change_rev']\n",
    "num_df=num_df.drop(['change_mou'], axis=1)\n",
    "num_df=num_df.drop(['change_rev'], axis=1)\n",
    "\n",
    "num_df=num_df.drop(['Customer_ID'], axis=1)\n",
    "\n",
    "churn=num_df['churn']\n",
    "num_df=num_df.drop(['churn'], axis=1)\n",
    "\n",
    "#no convertion\n",
    "\n",
    "num_df=num_df.fillna(conData)\n",
    "\n",
    "num_df[num_df < 0]=0\n",
    "\n",
    "result_df = pd.concat([num_df, cat_df], axis=1)\n",
    "np.nan_to_num(result_df)\n",
    "\n",
    "result_df_op=result_df\n",
    "#result_df_op=result_df_op.drop(['churn'], axis=1)\n",
    "#result_df_op=np.nan_to_num(result_df_op)\n",
    "\n",
    "X=result_df_op\n",
    "#y=result_df['churn'] \n",
    "y=churn\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.0, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_Mean</th>\n",
       "      <th>mou_Mean</th>\n",
       "      <th>totmrc_Mean</th>\n",
       "      <th>da_Mean</th>\n",
       "      <th>ovrmou_Mean</th>\n",
       "      <th>ovrrev_Mean</th>\n",
       "      <th>vceovr_Mean</th>\n",
       "      <th>datovr_Mean</th>\n",
       "      <th>roam_Mean</th>\n",
       "      <th>drop_vce_Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>infobase</th>\n",
       "      <th>HHstatin</th>\n",
       "      <th>dwllsize</th>\n",
       "      <th>ethnic</th>\n",
       "      <th>kid0_2</th>\n",
       "      <th>kid3_5</th>\n",
       "      <th>kid6_10</th>\n",
       "      <th>kid11_15</th>\n",
       "      <th>kid16_17</th>\n",
       "      <th>creditcd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75721</th>\n",
       "      <td>43.5650</td>\n",
       "      <td>695.75</td>\n",
       "      <td>44.9900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.50</td>\n",
       "      <td>8.5750</td>\n",
       "      <td>8.5750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80184</th>\n",
       "      <td>77.8025</td>\n",
       "      <td>1839.75</td>\n",
       "      <td>84.9900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.25</td>\n",
       "      <td>2.8125</td>\n",
       "      <td>2.8125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19864</th>\n",
       "      <td>35.4500</td>\n",
       "      <td>78.25</td>\n",
       "      <td>29.9900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.21</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76699</th>\n",
       "      <td>34.9900</td>\n",
       "      <td>30.00</td>\n",
       "      <td>44.9900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92991</th>\n",
       "      <td>32.2875</td>\n",
       "      <td>204.00</td>\n",
       "      <td>7.9975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.25</td>\n",
       "      <td>1.7000</td>\n",
       "      <td>1.7000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rev_Mean  mou_Mean  totmrc_Mean  da_Mean  ovrmou_Mean  ovrrev_Mean  \\\n",
       "75721   43.5650    695.75      44.9900      0.0        24.50       8.5750   \n",
       "80184   77.8025   1839.75      84.9900      0.0        11.25       2.8125   \n",
       "19864   35.4500     78.25      29.9900      0.0         0.00       0.0000   \n",
       "76699   34.9900     30.00      44.9900      0.0         0.00       0.0000   \n",
       "92991   32.2875    204.00       7.9975      0.0         4.25       1.7000   \n",
       "\n",
       "       vceovr_Mean  datovr_Mean  roam_Mean  drop_vce_Mean  ...  infobase  \\\n",
       "75721       8.5750          0.0       0.00       5.333333  ...         1   \n",
       "80184       2.8125          0.0       0.00      51.000000  ...         1   \n",
       "19864       0.0000          0.0       2.21       3.666667  ...         1   \n",
       "76699       0.0000          0.0       0.00       0.000000  ...         1   \n",
       "92991       1.7000          0.0       0.00      26.000000  ...         1   \n",
       "\n",
       "       HHstatin  dwllsize  ethnic  kid0_2  kid3_5  kid6_10  kid11_15  \\\n",
       "75721         3         1       6       1       1        1         1   \n",
       "80184         0         0      10       1       1        1         1   \n",
       "19864         0         1       4       1       1        1         1   \n",
       "76699         2         1      14       1       1        1         1   \n",
       "92991         4         1       6       1       1        1         1   \n",
       "\n",
       "       kid16_17  creditcd  \n",
       "75721         1         2  \n",
       "80184         1         1  \n",
       "19864         1         2  \n",
       "76699         1         1  \n",
       "92991         1         2  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8669 41769 6931 42631\n",
      "pod:  0.860154957427061\n",
      "pof:  0.8281256195725445\n",
      "AUC:  0.5160146689272582\n",
      "roc_auc:  0.5630052989047692\n",
      "auc:  0.5160146689272583\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Create a Gaussian Classifier\n",
    "classifier = GaussianNB()\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "auc = auc(fpr, tpr)\n",
    "print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28292 22146 23977 25585\n",
      "pod:  0.5162221056454542\n",
      "pof:  0.43907371426305564\n",
      "AUC:  0.5385741956911992\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(n_neighbors=5)  \n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28297 22141 19940 29622\n",
      "pod:  0.5976756385940841\n",
      "pof:  0.43897458265593403\n",
      "AUC:  0.5793505279690749\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier =RandomForestClassifier(n_estimators=100,max_depth=2,random_state=0)\n",
    "classifier.fit(X_train, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc:  0.579350527969075\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "auc = auc(fpr, tpr)\n",
    "print('auc: ',auc)\n",
    "plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('NO DT-ROC Curve for RF')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30114 20324 20552 29010\n",
      "pod:  0.5853274686251564\n",
      "pof:  0.40295015662793926\n",
      "AUC:  0.5911886559986086\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "71999/71999 [==============================] - 19s 268us/step - loss: 8.0436 - acc: 0.4955 - val_loss: 8.0341 - val_acc: 0.4961\n",
      "Epoch 2/30\n",
      "71999/71999 [==============================] - 15s 215us/step - loss: 8.0428 - acc: 0.4955 - val_loss: 8.0341 - val_acc: 0.4961\n",
      "Epoch 3/30\n",
      "71999/71999 [==============================] - 15s 214us/step - loss: 8.0428 - acc: 0.4955 - val_loss: 8.0341 - val_acc: 0.4961\n",
      "Epoch 4/30\n",
      "71999/71999 [==============================] - 16s 216us/step - loss: 8.0428 - acc: 0.4955 - val_loss: 8.0341 - val_acc: 0.4961\n",
      "y2_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  1.0\n",
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "71999/71999 [==============================] - 18s 255us/step - loss: 0.9829 - acc: 0.5039 - val_loss: 0.6935 - val_acc: 0.5039\n",
      "Epoch 2/30\n",
      "71999/71999 [==============================] - 17s 232us/step - loss: 0.6937 - acc: 0.5041 - val_loss: 0.6931 - val_acc: 0.5035\n",
      "Epoch 3/30\n",
      "71999/71999 [==============================] - 16s 218us/step - loss: 0.6936 - acc: 0.5028 - val_loss: 0.6935 - val_acc: 0.5038\n",
      "Epoch 4/30\n",
      "71999/71999 [==============================] - 16s 222us/step - loss: 0.6932 - acc: 0.5022 - val_loss: 0.6932 - val_acc: 0.4962\n",
      "Epoch 5/30\n",
      "71999/71999 [==============================] - 16s 217us/step - loss: 0.6984 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5039\n",
      "y2_pred:  [[0.49871877]\n",
      " [0.49871877]\n",
      " [0.49871877]\n",
      " ...\n",
      " [0.49871877]\n",
      " [0.49871877]\n",
      " [0.49871874]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.0\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 19s 263us/step - loss: 7.9862 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 16s 219us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 16s 225us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 16s 222us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "y2_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.0\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 19s 265us/step - loss: 7.9792 - acc: 0.5046 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 16s 225us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 16s 228us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 16s 227us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "y2_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.0\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 20s 274us/step - loss: 7.9865 - acc: 0.5044 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 17s 230us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 17s 230us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 16s 228us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "y2_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.0\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 21s 286us/step - loss: 8.0408 - acc: 0.4956 - val_loss: 8.0341 - val_acc: 0.4961\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 17s 232us/step - loss: 8.0427 - acc: 0.4955 - val_loss: 8.0341 - val_acc: 0.4961\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 17s 230us/step - loss: 8.0427 - acc: 0.4955 - val_loss: 8.0341 - val_acc: 0.4961\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 17s 231us/step - loss: 8.0427 - acc: 0.4955 - val_loss: 8.0341 - val_acc: 0.4961\n",
      "y2_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  1.0\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 21s 285us/step - loss: 8.0461 - acc: 0.4952 - val_loss: 8.0341 - val_acc: 0.4961\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 18s 249us/step - loss: 8.0427 - acc: 0.4955 - val_loss: 8.0341 - val_acc: 0.4961\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 17s 241us/step - loss: 8.0427 - acc: 0.4955 - val_loss: 8.0341 - val_acc: 0.4961\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 17s 242us/step - loss: 8.0427 - acc: 0.4955 - val_loss: 8.0341 - val_acc: 0.4961\n",
      "y2_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  1.0\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 22s 302us/step - loss: 7.9864 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 17s 241us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 17s 241us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 17s 240us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "y2_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.0\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 22s 302us/step - loss: 8.0007 - acc: 0.5036 - val_loss: 7.9341 - val_acc: 0.5077\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 19s 261us/step - loss: 8.0020 - acc: 0.5035 - val_loss: 7.9341 - val_acc: 0.5077\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 19s 261us/step - loss: 8.0020 - acc: 0.5035 - val_loss: 7.9341 - val_acc: 0.5077\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 19s 262us/step - loss: 8.0020 - acc: 0.5035 - val_loss: 7.9341 - val_acc: 0.5077\n",
      "y2_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.0\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 22s 306us/step - loss: 8.0033 - acc: 0.5034 - val_loss: 7.9341 - val_acc: 0.5077\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 18s 249us/step - loss: 8.0020 - acc: 0.5035 - val_loss: 7.9341 - val_acc: 0.5077\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 18s 251us/step - loss: 8.0020 - acc: 0.5035 - val_loss: 7.9341 - val_acc: 0.5077\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 18s 248us/step - loss: 8.0020 - acc: 0.5035 - val_loss: 7.9341 - val_acc: 0.5077\n",
      "y2_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.0\n",
      "pod:  [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
      "pof:  [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
      "auc:  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "tn mean:  3530.6\n",
      "fp mean:  1513.2\n",
      "fn mean:  3469.3\n",
      "tp mean:  1486.9\n"
     ]
    }
   ],
   "source": [
    "#FNN\n",
    "def get_FNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    #get number of columns in training data\n",
    "    n_cols = X2_train.shape[1]\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #compile model using mse as a measure of model performance\n",
    "    #model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  \n",
    "    from keras.callbacks import EarlyStopping\n",
    "    #set early stopping monitor so the model stops training when it won't improve anymore\n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "    #train model\n",
    "    model.fit(X2_train, y2_train, validation_split=0.2, epochs=30, callbacks=[early_stopping_monitor])\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    print('y2_pred: ',y2_pred)\n",
    "    y22_pred=y2_pred.round()\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(X_train,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=X_train.iloc[train_index], X_train.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_FNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "print ('tn mean: ',sum(tn_list) / len(tn_list))\n",
    "print ('fp mean: ',sum(fp_list) / len(fp_list))\n",
    "print ('fn mean: ',sum(fn_list) / len(fn_list))\n",
    "print ('tp mean: ',sum(tp_list) / len(tp_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 115s 2ms/step - loss: 0.6874 - acc: 0.5402 - val_loss: 0.6899 - val_acc: 0.5483\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 109s 2ms/step - loss: 0.6745 - acc: 0.5802 - val_loss: 0.6819 - val_acc: 0.5593\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 109s 2ms/step - loss: 0.6587 - acc: 0.6121 - val_loss: 0.6946 - val_acc: 0.5604\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 110s 2ms/step - loss: 0.6364 - acc: 0.6413 - val_loss: 0.7259 - val_acc: 0.5498\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 110s 2ms/step - loss: 0.6166 - acc: 0.6644 - val_loss: 0.7186 - val_acc: 0.5632\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.6594714545087754\n",
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 112s 2ms/step - loss: 0.6869 - acc: 0.5437 - val_loss: 0.6834 - val_acc: 0.5596\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 110s 2ms/step - loss: 0.6747 - acc: 0.5811 - val_loss: 0.6893 - val_acc: 0.5468\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 110s 2ms/step - loss: 0.6588 - acc: 0.6105 - val_loss: 0.6919 - val_acc: 0.5458\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 110s 2ms/step - loss: 0.6345 - acc: 0.6409 - val_loss: 0.6924 - val_acc: 0.5586\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 110s 2ms/step - loss: 0.6130 - acc: 0.6625 - val_loss: 0.7530 - val_acc: 0.5553\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7692152511599758\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 113s 2ms/step - loss: 0.6876 - acc: 0.5410 - val_loss: 0.6839 - val_acc: 0.5557\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 112s 2ms/step - loss: 0.6750 - acc: 0.5781 - val_loss: 0.6871 - val_acc: 0.5571\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 110s 2ms/step - loss: 0.6603 - acc: 0.6084 - val_loss: 0.6976 - val_acc: 0.5522\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 110s 2ms/step - loss: 0.6368 - acc: 0.6395 - val_loss: 0.6918 - val_acc: 0.5584\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 111s 2ms/step - loss: 0.6156 - acc: 0.6624 - val_loss: 0.7408 - val_acc: 0.5533\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.579499596448749\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 114s 2ms/step - loss: 0.6885 - acc: 0.5365 - val_loss: 0.6836 - val_acc: 0.5548\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 111s 2ms/step - loss: 0.6755 - acc: 0.5801 - val_loss: 0.6839 - val_acc: 0.5533\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 112s 2ms/step - loss: 0.6620 - acc: 0.6047 - val_loss: 0.6892 - val_acc: 0.5531\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 110s 2ms/step - loss: 0.6411 - acc: 0.6350 - val_loss: 0.6914 - val_acc: 0.5603\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 110s 2ms/step - loss: 0.6204 - acc: 0.6579 - val_loss: 0.6962 - val_acc: 0.5589\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.6527441485068604\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 113s 2ms/step - loss: 0.6873 - acc: 0.5401 - val_loss: 0.6857 - val_acc: 0.5502\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 110s 2ms/step - loss: 0.6753 - acc: 0.5811 - val_loss: 0.6810 - val_acc: 0.5629\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 110s 2ms/step - loss: 0.6586 - acc: 0.6098 - val_loss: 0.6939 - val_acc: 0.5602\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 110s 2ms/step - loss: 0.6328 - acc: 0.6435 - val_loss: 0.6850 - val_acc: 0.5715\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 111s 2ms/step - loss: 0.6112 - acc: 0.6663 - val_loss: 0.7184 - val_acc: 0.5616\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.5435835351089588\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 115s 2ms/step - loss: 0.6867 - acc: 0.5421 - val_loss: 0.6813 - val_acc: 0.5632\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 113s 2ms/step - loss: 0.6738 - acc: 0.5841 - val_loss: 0.6826 - val_acc: 0.5606\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 112s 2ms/step - loss: 0.6595 - acc: 0.6073 - val_loss: 0.6807 - val_acc: 0.5646\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 112s 2ms/step - loss: 0.6381 - acc: 0.6369 - val_loss: 0.7002 - val_acc: 0.5531\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 111s 2ms/step - loss: 0.6151 - acc: 0.6638 - val_loss: 0.7132 - val_acc: 0.5519\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.4737691686844229\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 114s 2ms/step - loss: 0.6874 - acc: 0.5425 - val_loss: 0.6824 - val_acc: 0.5624\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 112s 2ms/step - loss: 0.6743 - acc: 0.5818 - val_loss: 0.6810 - val_acc: 0.5667\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 112s 2ms/step - loss: 0.6599 - acc: 0.6098 - val_loss: 0.7060 - val_acc: 0.5475\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 111s 2ms/step - loss: 0.6403 - acc: 0.6381 - val_loss: 0.7475 - val_acc: 0.5541\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 112s 2ms/step - loss: 0.6197 - acc: 0.6599 - val_loss: 0.7120 - val_acc: 0.5574\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7092413236481033\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 115s 2ms/step - loss: 0.6879 - acc: 0.5390 - val_loss: 0.6833 - val_acc: 0.5584\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 112s 2ms/step - loss: 0.6755 - acc: 0.5781 - val_loss: 0.6826 - val_acc: 0.5597\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 112s 2ms/step - loss: 0.6603 - acc: 0.6079 - val_loss: 0.6852 - val_acc: 0.5532\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 112s 2ms/step - loss: 0.6387 - acc: 0.6384 - val_loss: 0.7013 - val_acc: 0.5516\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 112s 2ms/step - loss: 0.6189 - acc: 0.6606 - val_loss: 0.7038 - val_acc: 0.5631\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.6361985472154964\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 115s 2ms/step - loss: 0.6872 - acc: 0.5433 - val_loss: 0.6833 - val_acc: 0.5573\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 111s 2ms/step - loss: 0.6751 - acc: 0.5804 - val_loss: 0.6741 - val_acc: 0.5816\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 111s 2ms/step - loss: 0.6616 - acc: 0.6064 - val_loss: 0.6788 - val_acc: 0.5660\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 111s 2ms/step - loss: 0.6388 - acc: 0.6381 - val_loss: 0.7057 - val_acc: 0.5654\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 111s 2ms/step - loss: 0.6105 - acc: 0.6686 - val_loss: 0.7019 - val_acc: 0.5567\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.48062953995157387\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 115s 2ms/step - loss: 0.6865 - acc: 0.5427 - val_loss: 0.6778 - val_acc: 0.5739\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 113s 2ms/step - loss: 0.6677 - acc: 0.5931 - val_loss: 0.6825 - val_acc: 0.5658\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 113s 2ms/step - loss: 0.6499 - acc: 0.6215 - val_loss: 0.6932 - val_acc: 0.5475\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 113s 2ms/step - loss: 0.6329 - acc: 0.6462 - val_loss: 0.7030 - val_acc: 0.5534\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 113s 2ms/step - loss: 0.6172 - acc: 0.6630 - val_loss: 0.7361 - val_acc: 0.5462\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.5365213882163035\n",
      "pod:  [0.6594714545087754, 0.7692152511599758, 0.579499596448749, 0.6527441485068604, 0.5435835351089588, 0.4737691686844229, 0.7092413236481033, 0.6361985472154964, 0.48062953995157387, 0.5365213882163035]\n",
      "pof:  [0.5394528152260111, 0.6518636003172086, 0.47145122918318794, 0.5477795400475813, 0.4113798572561459, 0.38223632038065025, 0.5955590800951626, 0.5067406819984139, 0.37140590918104305, 0.4263335316279992]\n",
      "auc:  [0.5600093196413822, 0.5586758254213837, 0.5540241836327806, 0.5524823042296395, 0.5661018389264065, 0.5457664241518864, 0.5568411217764704, 0.5647289326085412, 0.5546118153852654, 0.5550939282941522]\n",
      "tn mean:  2570.2\n",
      "fp mean:  2473.6\n",
      "fn mean:  1962.2\n",
      "tp mean:  2994.0\n"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "def get_RNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    import pandas as pd\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense,Dropout, LSTM, GRU\n",
    "    from keras.layers import Embedding\n",
    "    max_features = 400000 # number of words to consider as features\n",
    "    import numpy as np\n",
    "    #create model \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 32))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X2_train, y2_train, epochs=5, batch_size=128, validation_split=0.2)\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    y22_pred=y2_pred.round()\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(X_train,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=X_train.iloc[train_index], X_train.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_RNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    \n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "print ('tn mean: ',sum(tn_list) / len(tn_list))\n",
    "print ('fp mean: ',sum(fp_list) / len(fp_list))\n",
    "print ('fn mean: ',sum(fn_list) / len(fn_list))\n",
    "print ('tp mean: ',sum(tp_list) / len(tp_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN - 1D CNN\n",
    "def get_CNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras import layers\n",
    "    from keras.optimizers import RMSprop\n",
    "    max_features = 400000 # number of words to consider as features\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(max_features, 128, input_length=96))\n",
    "    model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(5))\n",
    "    model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(1))\n",
    "    model.summary()\n",
    "    model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X2_train, y2_train, epochs=5, batch_size=128, validation_split=0.2)\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    y22_pred=y2_pred.round()\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(X_train,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=X_train.iloc[train_index], X_train.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_CNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "print ('tn mean: ',sum(tn_list) / len(tn_list))\n",
    "print ('fp mean: ',sum(fp_list) / len(fp_list))\n",
    "print ('fn mean: ',sum(fn_list) / len(fn_list))\n",
    "print ('tp mean: ',sum(tp_list) / len(tp_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
